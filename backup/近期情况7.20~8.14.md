因7.29至8.11有其他相关项目，故进度略缓慢，抱歉
本页面所编写代码文件：[search_v2.zip](https://github.com/user-attachments/files/16608054/search_v2.zip)

# 近期实习情况
## 1、项目需求
要求根据用户输入的行业名称，自动生成搜索关键词，然后爬取并下载相关网页，总结获取到的所有内容。
## 2、具体流程
根据需求，我将其拆作以下数个部分：
#### （1）接受用户输入的行业名称，输入到本地模型中并返回相关关键词
#### （2）通过返回的关键词，爬取并下载网页
#### （3）通过本地模型总结网页内容
## 3、实现方法
我在Github上寻找了些许开源项目，并将其用于构建本项目。
目前主要使用了ScrapeGraphAI，一个网络爬虫Python库。它可以方便地从网页中提取、总结信息。
### 3.1、Ollama
ScrapeGraphAI库目前没有对LM Studio的充足支持，而使用Ollama相对方便，因此需要先安装Ollama并安装相关模型，流程如下：
#### （1）下载并安装Ollama：[下载链接](https://ollama.com/download/OllamaSetup.exe)
#### （2）通过命令指示符安装llama3和嵌入模型nomic-embed-text：
```cmd
ollama pull llama3
ollama pull nomic-embed-text
```
#### （3）Ollama自动在后台运行，默认端口为11434
### 3.2、使用ScrapeGraphAI
安装ScrapeGraphAI：
```cmd
pip install scrapegraphai
```
引入库并定义ScrapeGraphAI相关参数：
```python
import requests, time, gradio
from bs4 import BeautifulSoup
from scrapegraphai.graphs import SmartScraperGraph as SSG

graph_config = {
    "llm": {
        "model": "ollama/llama3",
        "temperature": 0.4,
        "format": "pdf",
        "base_url": "http://localhost:11434",
    },
    "embeddings": {
        "model": "ollama/nomic-embed-text",
        "base_url": "http://localhost:11434",
    },
    "verbose":True,
}
```

制作一个获取页面链接的爬虫：
```python
def get_url(keyword):
    search_engine_url = "https://www.baidu.com/s"
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

    if keyword:
        pass
    else:
        keyword="头豹"

    max_links=5
    search_url = f"{search_engine_url}?wd={keyword}"

    headers = {
        "User-Agent": user_agent,
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0"
    }
    
    try:
        response = requests.get(search_url, headers=headers)
        time.sleep(2)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            urls = []
            for url in soup.select('.t a'):
                link = url.get('href')
                if link:
                    urls.append(link)
                if len(urls) >= max_links:
                    break
            problem=""
            return urls, problem
        else:
            urls=[]
            problem=f"获取网页失败：{response.status_code}"
            return urls, problem
    except requests.RequestException as e:
        urls=[]
        problem=f"请求出错: {e}"
        return urls, problem
```

结合以上爬虫与ScrapeGraphAI:
```python
def main(keyword,prompt):
    urls = get_url(keyword)[0]
    problem = get_url(keyword)[1]
    primary_results=[]
    if urls:
        for url in urls:
            smart_scraper_graph = SSG(
                prompt=prompt,
                source=url,
                config=graph_config
        )
            result = smart_scraper_graph.run()
            primary_results.append(result)
            results = remove_blanks(primary_results)
        return urls, results
    else:
        results = "未获取页面，无法解析"
        return problem, results
```

再通过gradio做一个简单的界面：
```python
def gradio_gui():
    keyword_input=gradio.Textbox(lines=5, label="搜索关键词")
    prompt_input=gradio.Textbox(lines=10, label="大模型提示词")
    url_output=gradio.Textbox(lines=5, label="搜索网页结果")
    results_output=gradio.Textbox(lines=10, label="大模型总结")
    interface=gradio.Interface(
        fn=main,
        inputs=[keyword_input,prompt_input],
        outputs=[url_output,results_output],
        title="页面爬取&大模型总结",
        description="输入搜索关键词和大模型提示词"
    )
    interface.launch(share=False)
```
运行：
```python
gradio_gui()
```
### 页面如下：
![微信截图_20240814114703](https://github.com/user-attachments/assets/977b3e31-f843-4c0e-b2fe-feade69576ec)

### 测试效果：
![微信截图_20240814114750](https://github.com/user-attachments/assets/6be384c1-434d-402a-86f1-371625c4ae7a)
### 3.3、显著问题及未来方向
以上输出包含五个页面链接但只有一个输出结果。究其原因，是搜出的页面不少都具有反爬取功能，大模型无法阅读页面内的信息。此外，本程序提示词尚不完善，且欠缺储存页面和储存输出结果的能力。综上，未来方向暂定如下：
#### （1）、改进爬虫，针对反爬网站进行优化
#### （2）、测试并改进提示词，优化大模型表现
#### （3）、存储页面到本地，构建知识库
