# 近期实习情况
本项目所涉及代码：
项目1：[嵌入项目](https://github.com/user-attachments/files/16906725/doc_RAG1.zip)
项目2：[提示词优化](https://github.com/user-attachments/files/16906731/SSSimproved.zip)
项目3：本项目不涉及完整代码，请参考本文档对项目1的代码进行修改。
# 目录

- [项目需求](#-1、项目需求)
- [具体流程](#-2、具体流程)
- [实现方法](#-3、实现方法)
  - [嵌入项目](##-3.1、嵌入项目)
    - [下载嵌入模型](###-3.1.1、下载嵌入模型)
    - [根据关键词下载文本](###-3.1.2、根据关键词从网络上下载文本)
    - [使用嵌入模型加载文本](###-3.1.3、使用嵌入模型加载文本)
    - [构建问答链和聊天函数](###-3.1.4、构建问答链和聊天函数)
    - [运行](###-3.1.5、运行)
  - [提示词项目](##-3.2、提示词项目)
    - [优化部分](##-3.2.1、优化部分)
    - [优化效果](##-3.2.2、优化效果简评)
  - [嵌入排名](##-3.3、嵌入排名项目)

# 1、项目需求
提示词方式依据搜索到的回答，并标注观点来源
给一个（能有多个效果会更好）回答的示例，并在给予一个标准对照，然后打分
用embedding排序结果

# 2、具体流程
根据需求，我将其拆作以下数个部分：
#### （1）将页面内容嵌入后，程序根据页面做出回答，之后返回相关页面。
#### （2）通过提示词，给予模型数个满分示例和打分标准，最后排名。
#### （3）通过定义相关参数（如similarity score），可以通过向量相似度打分并排名。

# 3、实现方法
## 3.1、嵌入项目
嵌入模型负责将文本转换为向量，有利于捕捉语义信息和特征，并用于后续的检索问答。
### 3.1.1、下载嵌入模型
在HuggingFace官网上搜索用于中文的文本嵌入模型，本项目目前暂时使用text2vec3。
下载地址：[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)

**请将模型路径粘贴至embedding_model_dict中
"text2vec3": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/text2vec-base-chinsese"的
"PATH/TO/YOUR/DICT/EMBEDDINGDICT/text2vec-base-chinsese"部分。（替换原内容）**

### 3.1.2、根据关键词从网络上下载文本

**请将下载路径粘贴至以下代码 txt_path = "PATH/TO/YOUR/DICT/txt_DICT"的
"PATH/TO/YOUR/DICT/txt_DICT"部分。（替换原内容）**

```python
txt_path = "PATH/TO/YOUR/DICT/txtDICT"

headers = {
    "User-Agent": UserAgent().random,
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0"
}

def get_urls(keyword):
    search_url = f"https://www.baidu.com/s?wd={keyword}"    
    try:
        response = requests.get(search_url, headers=headers)
        time.sleep(random.random() * 4)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            urls = []
            for url in soup.select(".t a"):
                link = url.get("href")
                if link:
                    urls.append(link)
                if len(urls) >= 3:
                    break
            return urls, ""
        
        else:
            return [], f"获取链接失败：{response.status_code}"
        
    except requests.RequestException as e:
        return [], f"请求出错：{e}"

def clean_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    for tag in soup(['script', 'style', 'img', 'video', 'audio']):
        tag.decompose()
    for tag in soup.find_all(class_=lambda c: c and ('ad' in c or 'advert' in c)):
        tag.decompose()
    text = soup.get_text()
    cleaned_text = ' '.join(text.split())
    return cleaned_text

def save_to_txt(filename, content, path="./"):
    full_path = os.path.join(path, filename)
    os.mkdir(os.path.dirname(full_path))
    with open(full_path, 'w', encoding='utf-8') as file:
        file.write(content)

def fetch_and_clean(url):
    global output_path
    try:
        response = requests.get(url)
        if response.status_code == 200:
            html_content = response.text
            cleaned_text = clean_html(html_content)
            save_to_txt('cleaned_content.txt', cleaned_text, path=txt_path)
            print(f"内容已保存到{txt_path}/cleaned_content.txt")
        else:
            print(f"获取网页失败：{response.status_code}")
    except requests.RequestException as e:
        print(f"请求出错: {e}")

url = get_urls(keyword=input("关键词输入："))
fetch_and_clean(url = url)
```
### 3.1.3、使用嵌入模型加载文本
通过加载文件并分割成更小的段落，使用嵌入模型将文本转换为高维向量并储存在本地：
```python
prompt_template = """根据文档中的已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，不允许在答案中添加编造成分，答案请使用中文。\n
文档：{context}\n
问题: {question}
"""
prompts = PromptTemplate(
    template=prompt_template, input_variables=["context","question"]
)
chain_type_kwargs = {"prompt": prompts}
text_loader_kwargs = {"autodetect_encoding": True}
llm = ChatOllama(model="qwen:7b")

embedding_model_dict = {
    "ernie-tiny": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/ernie-3.0-nano-zh",
    "ernie-base": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/ernie-3.0-base-zh",
    "text2vec": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/text2vec-large-chinese",
    "text2vec2": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/sbert-base-chinese-nli",
    "text2vec3": "PATH/TO/YOUR/DICT/EMBEDDINGDICT/text2vec-base-chinsese"
}

def load_txts(directory = txt_path):
    Dloader = DirectoryLoader(directory, loader_cls = TextLoader, loader_kwargs = text_loader_kwargs, use_multithreading = True)
    docs = Dloader.load()
    splitter = CharacterTextSplitter(chunk_size = 150, chunk_overlap = 20)
    split_docs = splitter.split_documents(docs)
    return split_docs

try:
    def load_embedding_model(model_name="text2vec3"): 
        encode_kwargs = {"normalize_embeddings": False}
        model_kwargs = {"device": "cuda:0"}
        return HuggingFaceEmbeddings(
            model_name = embedding_model_dict[model_name],
            model_kwargs = model_kwargs,
            encode_kwargs = encode_kwargs
        )
    embeddings = load_embedding_model("text2vec3")
except AssertionError:
    
    def load_embedding_model(model_name="ernie-tiny"): 
        encode_kwargs = {"normalize_embeddings": False}
        model_kwargs = {"device": "cpu"}
        return HuggingFaceEmbeddings(
            model_name = embedding_model_dict[model_name],
            model_kwargs = model_kwargs,
            encode_kwargs = encode_kwargs
        )
    embeddings = load_embedding_model("text2vec3")
def store_chroma(docs, embeddings, persist_directory = "PATH/TO/YOUR/DICT/VECTORSTROEDICT/"):
    db = Chroma.from_documents(docs, embeddings, persist_directory = persist_directory)
    db.persist()
    return db

if not os.path.exists("PATH/TO/YOUR/DICT/VECTORSTROEDICT/"):
    os.makedirs("PATH/TO/YOUR/DICT/VECTORSTROEDICT/")
    documents = load_txts()
    db = store_chroma(documents, embeddings)
else:
    db = Chroma(persist_directory = "PATH/TO/YOUR/DICT/VECTORSTROEDICT/", embedding_function = embeddings)

retriever = db.as_retriever(search_type="similarity")
```
### 3.1.4、构建问答链和聊天函数
也就是应用嵌入好的文本文档，让大模型根据这些文档回答问题。如果文档中没有相关信息，AI就会回答“根据已知信息无法回答该问题”；如果有，AI就会根据已知信息进行回答，并返回相关的页面内容。
```python
qa = RetrievalQA.from_chain_type(
    llm = llm,
    chain_type = "stuff",
    retriever = retriever,
    chain_type_kwargs = chain_type_kwargs,
    return_source_documents = True
)

def chat(query):
    result = qa.invoke({"query":query})
    response = str(result["result"]) + "\n\n参考资料：\n" + str(result["source_documents"])
    return response

while True:
    chat(query = input("问题：") )
```
### 3.1.5、运行
输入关键词后自动爬取页面并下载到本地，然后读取本地文档，存储为向量数据库，自动创建检索器和问答链，并开始聊天。

## 3.2、提示词项目
本项目仅在上一个项目的提示词方面有所优化。
### 3.2.1、优化部分
对该部分代码：
```python
ranking_pages_messages = [
    ("system", "你是一位非常优秀的分析师。你能够根据网页内容，判断该页面与{vocation}是否有关系。"),
    ("human", "以下是数个页面的页面内容，以列表形式给出：\n{web_content}\n这些页面与{vocation}的相关程度高吗？请给它们的相关程度排名。"),
    ("ai", "好的，以下是这些页面的相关程度排名：")
]
```
在“system”处加入了案例，以供AI参考。
```python
ranking_pages_messages = [
    ("system", "你是一位非常优秀的分析师。你能够根据网页内容，判断该页面与关键词“{vocation}”是否有关系，并进一步给出相关得分，满分为100，分值必须为整数。\n举个例子：对于关键词“分析师”一个页面的内容为：“分析师，经济学术语，有很多种类，如CFA注册金融分析师、金融分析师、投资分析师、证券分析师、RCA分析师等。注册特许分析师-RCA。注册特许分析师公会(The Association of Registered Chartered Analysts，简称Association of RCA)在1999年(己卯年)成立，是全球首个集合管理、财务、金融、商业多领域的分析师团体，总部设在美国，RCA公会为制定高标准的专业道德准则（Code of Ethics）、教育（Education）被众多国家和地区的企业所认可和采用。公会在全球首创使用并颁发“RCA注册特许分析师”和“FRCA资深注册特许分析师”资格认证职衔。全球会员超过15000人，主要分布美国、加拿大等北美地区。金融分析师是证券投资与管理界的一种职业资格称号，CFA是“注册金融分析师”(CharteredFinancialAna-lyst)的简称的缩写，它是证券投资与管理界的一种职业资格称号，他们分布在证券公司、商业银行、保险公司以及投资机构。由美国“注册金融分析师学院”(ICFA)发起成立。该学院最初是在1959年6月由美国“金融分析师联合会”(FAF)同意在弗吉尔亚的夏洛茨维尔市与弗尔市与弗吉尼亚大学联合设立。”该页面和“分析师”的相关度非常高，给出了详细的定义、介绍、分类等相关信息，但是是信息量不够多，故得分为97。而对于另一个页面“12月5日，被誉为“券商界奥斯卡”的2023第二十一届新财富最佳分析师评选结果正式出炉。其中，宏观经济研究领域最佳分析师由广发证券郭磊研究小组获得；浙商证券李超、华创证券张瑜、国盛证券熊园、天风证券宋雪涛等研究小组分列其后。策略研究领域，最佳分析师榜首为天风证券刘晨明研究小组。此后依次是长江证券包承超、申万宏源证券王胜、广发证券戴康、兴业证券张启尧等研究小组。固收研究领域，华泰证券张继强继续稳坐榜首，从2019年至今，已蝉联五届第一名。金融工程领域，兴业证券郑兆磊研究小组排名第一。澎湃新闻记者统计显示，此次新财富公布的包括宏观经济、策略研究、固定收益研究、金融工程、银行等在内30个研究领域，长江证券和广发证券成为大赢家，合计均上榜20次，双双位居头名。兴业证券紧随其后，合计上榜14次。天风证券上榜13次，申万宏源证券则上榜10次。国盛证券、海通证券则均上榜9次。”,该页面和“分析师”有关，但是没有给出相关介绍，只是相关的新闻内容，故得分为73。"),
    ("human", "以下是数个页面的页面内容，以列表形式给出，页面列表中的位置代表它是哪个页面的页面内容：\n{web_content}\n这些页面与{vocation}的相关程度高吗？请给它们相关程度的得分，并根据得分给出排名。"),
    ("ai", "好的，以下是这些页面的相关程度的得分和排名：")
]
```
### 3.2.2、优化效果
优化过后，AI能够针对每个页面给出相关的得分，并根据得分给出排名。其排名效果相较原先版本而言有极大改善，主要体现在几个方面：
1、旧版本排名容易产生错误理解，例如将对单个页面内的内容针对多部分进行评分；新版本可以更准确地针对页面而非其中的某一部分。
2、旧版本的排名没有依据，产生的结果随机性很大；新版本有了两个案例，随机性有所收敛。
3、旧版本的排名只显示页面间的相关度；新版本加入评分机制，可以显示页面和关键词的实际相关度。

## 3.3、嵌入排名项目
类似于上两个版本的结合，由于向量间本身就可以计算相似度，因此提示词会更简短。
**修改部分：**
将
```python
retriever = db.as_retriever(search_type="similarity")
```
修改为
```python
retriever = db.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 3,
        "score_threshold": 0.5
        }
) 
```
同时，将
```python
prompt_template = """根据文档中的已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，不允许在答案中添加编造成分，答案请使用中文。\n
文档：{context}\n
问题: {question}
"""
```
修改为
```python
prompt_template = """根据文档和关键词，为每份文档与关键词之间的相关度排名。排名仅与similarity_score的大小相关，越大代表约相关。\n
文档：{context}\n
关键词: {question}
"""
```