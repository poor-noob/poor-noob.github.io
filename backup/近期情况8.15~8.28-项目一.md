# 近期实习情况
近期主要进行两个项目：
1. 通过大模型总结行业信息的程序
2. 文本嵌入后进一步建立本地知识库

**本项目相关程序：**
[行业信息总结.zip](https://github.com/user-attachments/files/16768157/SearchScrapeSummarizeFiles.zip)
[爬虫+文本嵌入+本地知识库程序.zip](https://github.com/user-attachments/files/16766494/simple_rag.zip)
[更稳定的本地知识库建立，但不包含爬虫.zip](https://github.com/user-attachments/files/16768116/doc_RAG.zip)
## 1、通用部分
由于两个项目均使用爬虫相关代码，故最先介绍。
### 1.1、引入库，设置全局变量
requests用于向页面发送请求，random和time用于模拟延迟，UserAgent用于伪装请求头，BeautifulSoup用于解析页面内容。
全局变量headers用于之后的爬虫请求头设置。
```python
import requests, random, time
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

headers = {
    "User-Agent": UserAgent().random,
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0"
}
```
### 1.2、根据关键词获取页面链接
应用requests和BeautifulSoup库进行相关处理，通过搜索引擎获取与关键词相关的URL列表。
```python
def get_urls(keyword, max_links):
    search_url = f"https://www.baidu.com/s?wd={keyword}"    
    try:
        response = requests.get(search_url, headers=headers)
        time.sleep(random.random() * 4) 

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            urls = []
            for url in soup.select(".t a"):
                link = url.get("href")
                if link:
                    urls.append(link)
                if len(urls) >= max_links:
                    break
            return urls, ""
        
        else:
            return [], f"获取链接失败：{response.status_code}"
        
    except requests.RequestException as e:
        return [], f"请求出错：{e}"
```
### 1.3、根据获取到的链接，读取页面内容
原理同上，但是需要获取页面内的具体内容，且需要去除广告、图片、视频等无效内容。
```python
def scrape_text(urls):
    web_content = []
    for number, url in enumerate(urls, start = 1):
        try:
            html_content = requests.get(url, headers = headers).text
            time.sleep(random.random() * 4)
            
            rough_text = BeautifulSoup(html_content, "html.parser")
            title = rough_text.title.string
            for tag in rough_text(['script', 'style', 'img', 'video', 'audio']):
                tag.decompose()
            for tag in rough_text.find_all(class_=lambda c: c and ('ad' in c or 'advert' in c)):
                tag.decompose()
            cleaned_text = ' '.join(rough_text.get_text().split())
            web_content.append(cleaned_text)
            print(f"页面{number}的内容已读取。\n")

        except requests.RequestException as e:
            print(f"页面{number}的内容读取失败: {e}\n")

    return web_content
```
这样，一个简单的根据关键词进行页面查找和读取的爬虫就完成了。
## 2、项目一：行业信息总结
由于ScrapeGraphAI的官方文档较不详细，且其自带的爬虫爬取能力并不理想，故选择弃用ScrapeGraphAI。
目前选择使用Langchain+Ollama的组合实现行业信息总结的项目需求。
上文中已有的代码及功能将不再赘述，有区别的地方会特别说明。
### 2.1、引入库，设置全局变量
gradio用于制作交互界面，ChatPromptTemplate用于撰写提示词模板，OllamaLLM用于链接Ollama的语言模型。
四份不同的messages提示词模板分别用于不同功能：行业分析、关键词生成、页面内容总结、相关度排序。
```python
import re
import gradio as gr
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

msg = ""
rank = ""
urls_md = ""
keyword_list = []
model = OllamaLLM(model="qwen:7b")

analyze_messages = [
    ("system", "你是一位非常优秀的分析师。你擅长从八大维度分析一个行业：1、行业定义；2、行业分类；3、行业特征；4、发展历程；5、产品链分析；6、市场规模；7、政策分析；8、竞争格局"),
    ("human", "有一个和{vocation}有关的行业，请你尝试从八大维度分析这个行业。你需要针对每个维度总结一个相关短语，每个短语不超过15个字。"),
    ("ai", "好的，我将会从八大维度对该行业进行分析，并针对每个维度总结一个相关短语：")
]

keyword_messages = [
    ("system", "你是一位非常优秀的分析师。你擅长根据已有的对某个行业的简要分析，给出数个最相关的关键词。"),
    ("human", "有一段关于{vocation}相关行业的简要分析: {analysis}，请你给出任意个最相关的关键词。在你思考关键词的时候请考虑到：该关键词将被输入至搜索引擎内，请确保关键词较容易被搜索到。你的回答只需要包含关键词，关键词请用引号括起来。"),
    ("ai", "好的，我将会根据已有的分析，给出三个最相关的关键词：")
]

summarize_pages_messages = [
    ("system", "你是一位非常优秀的助手。你擅长提取文字中的关键信息，并用易懂的语言进行表达。"),
    ("human", "这个页面有什么与{vocation}相关的内容？请为我用中文简要总结。如果你读取到的页面几乎不包含相关的有效内容，只要回复我“该页面似乎没有相关内容”即可。"),
    ("human", "页面内容：{page_content}"),
    ("ai", "好的，以下是该页面的相关总结：")
]

ranking_pages_messages = [
    ("system", "你是一位非常优秀的分析师。你能够根据网页内容，判断该页面与{vocation}是否有关系。"),
    ("human", "以下是数个页面的页面内容，以列表形式给出：\n{web_content}\n这些页面与{vocation}的相关程度高吗？请给它们的相关程度排名。"),
    ("ai", "好的，以下是这些页面的相关程度排名：")
```
### 2.2、创建问答链
通过相同模板函数+不同提示词创建不同的问答链，用途上文已写。
```python
def thinking_chain(messages):
    prompt = ChatPromptTemplate.from_messages(messages)
    chain = prompt | model
    return chain

analyze_chain = thinking_chain(messages=analyze_messages)
keyword_chain = thinking_chain(messages=keyword_messages)
summarize_chain = thinking_chain(messages=summarize_pages_messages)
ranking_chain = thinking_chain(messages=ranking_pages_messages)
```
### 2.3、激活问答链
通过定义相关的函数应用已有问答链。
获取搜索关键词的函数：
```python
def get_keyword_list(vocation):
    analysis = analyze_chain.invoke({"vocation": vocation})
    print(analysis)

    keyword_string = keyword_chain.invoke({"vocation": vocation, "analysis": analysis})
    print(keyword_string)

    keyword_list = re.findall(r'"(.*?)"', keyword_string)
    keyword_list.append(vocation)
    print(keyword_list)

    return keyword_list
```
获取页面总结和相关度排序的函数：
```python
def chat(vocation, search_engine_url, max_links, keyword_list):
    global msg, rank, urls_md

    if not keyword_list:
        keyword_list.append(vocation)

    for keyword in keyword_list:
        print("当前关键词为：" + keyword + "\n")
        urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)

        for retries in range(3):
            if urls:
                web_content, urls_md = scrape_text(urls)
                print(f"为您找到{len(urls)}个页面！")
                msg += f"关键词：{keyword}\n为您找到{len(urls)}个页面！\n"

                for number, page_content in enumerate(web_content, start=1):
                    response = summarize_chain.invoke({"vocation": vocation, "page_content": page_content})
                    print(f"相关页面{number}的内容：\n" + str("\n".join(web_content)) + "\n\n\n")
                    msg += f"{keyword}相关页面{number}的内容总结：\n\n" + response + "\n\n"
                    print(f"相关页面{number}的内容总结：\n\n" + response + "\n")

                rank = ranking_chain.invoke({"vocation": vocation, "web_content": web_content})
                print(f"相关页面排名：\n{rank}\n")

                break

            elif problem:
                urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)
                msg += problem + "重试中...\n"
                print(problem + "重试中...")

            else:
                urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)
                msg += "获取链接失败：未知错误。重试中...\n"
                print("获取链接失败：未知错误。重试中...")

            if retries == 2:
                msg += "错误：当前关键词重试次数过多\n"
                print("错误：当前关键词重试次数过多")
                urls_md += f"[Null](https://127.0.0.1)\n"
                return msg, rank, urls_md
            
    return msg, rank, urls_md
```
### 2.4 交互界面制作
封装上述代码中定义的函数，返回到gradio界面中实现交互。
```python
with gr.Blocks() as gradio_gui:
    def search_engine_url_options():
        return ["baidu", "bing", "google"]
    
    with gr.Column():
        search_engine_url_choices = gr.Dropdown(choices=search_engine_url_options(), label="搜索引擎：", value="baidu")

        search_vocation = gr.Textbox(label="请输入您要搜索的行业：", value="分析师")

        search_max_links = gr.Slider(minimum=1, maximum=10, step=1, label="每个关键词的搜索页面数：")

        start_keyword_generate_button = gr.Button("生成关键词")

        search_keyword = gr.CheckboxGroup(label="相关关键词：", choices=["金融服务业", "数字化进程", "政策利率资本管制", "分析师"])
        
        with gr.Row():
            start_search_button = gr.Button("开始检索")

            clear_button = gr.Button("清空")

        with gr.Row():
            with gr.Column(scale=3):
                search_response = gr.Textbox(label="检索结果：", lines=20, placeholder="这里显示检索结果和总结。", visible=True)

            with gr.Column(scale=1):
                search_rank = gr.Textbox(label="页面度排名：", lines=10, placeholder="这里显示页面度排名。")

        search_urls = gr.Textbox(label="相关链接：", lines=10, placeholder="这里显示相关的页面链接。", visible=True)

        def clear_response():
            return gr.update(value="", visible=False)

        def generate_keyword(search_vocation):
            keyword_list = get_keyword_list(search_vocation)
            return gr.update(choices=keyword_list)
        
        def processing(message, search_engine_url, search_max_links, keyword_list):
            response, rank, urls_md = chat(message, search_engine_url, search_max_links, keyword_list)
            return response, rank, urls_md
        
    start_keyword_generate_button.click(generate_keyword, inputs=search_vocation, outputs=search_keyword)
    start_search_button.click(processing, inputs=[search_vocation, search_engine_url_choices, search_max_links, search_keyword], outputs=[search_response, search_rank, search_urls])
    clear_button.click(clear_response, outputs=[search_response, search_rank, search_urls])

gradio_gui.launch()
```
这样，一个用于行业相关内容检索的小程序就完成了，以下为界面实例：
#### 基础界面：
![image](https://github.com/user-attachments/assets/6db2a95c-c97c-4182-8c72-f2ae574ef9cb)
#### 可自定义搜索引擎：
<img width="487" alt="234f75ad343b4e2bdaf2d6c5487809b" src="https://github.com/user-attachments/assets/237fefea-e348-4422-86bc-9e211994b47a">

#### 返回页面总结、页面排名及超链接：
<img width="1103" alt="5d2700744b7407d3aef92a3283da719" src="https://github.com/user-attachments/assets/08fa9de0-454a-426d-9fa3-d00bfeae7fd0">

### 2.5、简要评价
该程序目前在稳定运行方面尚有欠缺，爬虫的性能依然存在相当的改进空间。此外，提示词的撰写也并非完美，对页面相关度的排序还有相当的优化空间。

## 3、项目二：嵌入+本地知识库建立
本项目来源于项目一的最初解决方案。由于运行效率和运行效果都较为一般，现已不作为项目一的解决方案进行开发，而是作为另一方向上的探索，以供未来参考。
### 3.1、引入库，设置全局变量
os用于执行对计算机的相关操作，如创建、写入目录。
DirectoryLoader用于读取文件路径，TextLoader用于阅读文件本身。
RetrievalQA用于创建检索链。
CharacterTextSplitter用于切分文件中的大段文字，便于进一步处理。
HuggingFaceEmbeddings用于使用从HuggingFace上下载的本地嵌入模型进行文本嵌入。
ChatOllama用于链接Ollama的语言模型进行对话。
Chroma用于储存嵌入文本，创建向量库。
path_dict定义文档读取路径和向量库存储路径，embedding_model_dict定义嵌入模型路径。
```python
import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma

llm = ChatOllama(model="qwen:7b")

path_dict = {
    "docs_path": "Path/To/Your/Dict/docs",
    "chroma_path": "Path/To/Your/Dict/VectorStore"
}

embedding_model_dict = {
    "ernie-tiny": "Path/To/Your/Dict/ernie-3.0-nano-zh",
    "ernie-base": "Path/To/Your/Dict/ernie-3.0-base-zh",
    "text2vec": "Path/To/Your/Dict/text2vec-large-chinese",
    "text2vec2": "Path/To/Your/Dict/sbert-base-chinese-nli",
    "text2vec3": "Path/To/Your/Dict/text2vec-base-chinsese"
}
```
### 3.2、根据页面链接获取页面内容并下载为txt文档
```python
def scrape_clean_download(urls, download_path):
    number = 1
    messages = []
    for url in urls:
        try:
            html_content = requests.get(url, headers = headers).text
            time.sleep(random.random()*5)
            rough_text = BeautifulSoup(html_content, "html.parser")
            for tag in rough_text(['script', 'style', 'img', 'video', 'audio']):
                tag.decompose()
            for tag in rough_text.find_all(class_=lambda c: c and ('ad' in c or 'advert' in c)):
                tag.decompose()
            cleaned_text = ' '.join(rough_text.get_text().split())
            path = os.path.join(download_path, f"webpage{number}.txt")
            try:
                with open(path,"x",encoding="utf-8") as file:
                    file.write(cleaned_text)
                messages.append(f"网页{number}的内容已保存到{path}。")
            except FileExistsError:
                messages.append(f"{path}文件已存在，无法保存。")
            finally:
                number += 1
        except:
            pass
    return "\n".join(messages)

def part_get_docs(keyword):
    if get_url(keyword = keyword)[0]:
        return scrape_clean_download(urls = get_url(keyword = keyword)[0], download_path = path_dict["docs_path"])
    else:
        return get_url(keyword = keyword)[1]
```
### 3.3、切分文档
```python
def split_docs():
    docs = DirectoryLoader(path_dict["docs_path"], loader_cls = TextLoader, loader_kwargs = {"autodetect_encoding": True}, use_multithreading=True)
    splited_docs = CharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200).split_documents(docs.load())
    return splited_docs
```
### 3.4、设置嵌入模型，建立向量库
```python
def set_embedding(model_name="text2vec"):
    try:
        embedding = HuggingFaceEmbeddings(
                model_name=embedding_model_dict[model_name],
                model_kwargs={"device": "cuda:0"},
                encode_kwargs={'normalize_embeddings': False}
            )
    except AssertionError as e:
        embedding = HuggingFaceEmbeddings(
                model_name=embedding_model_dict[model_name],
                model_kwargs={"device": "cpu"},
                encode_kwargs={'normalize_embeddings': False}
            )
    return embedding

def store_chroma(docs):
    db = Chroma.from_documents(documents = docs, embedding = set_embedding(), persist_directory = path_dict["chroma_path"])
    db.persist()
    return db
```
### 3.5、整合前两步内容并创建检索链
```python
def part_RAG_LLM(question):
    if not os.path.exists(path_dict["chroma_path"]):
        os.mkdir(path_dict["chroma_path"])
        db = store_chroma(docs = split_docs())
    else:
        db = Chroma(embedding_function = set_embedding(), persist_directory = path_dict["chroma_path"])

    retriever = db.as_retriever(
        search_type = "similarity_score_threshold",
        search_kwargs = {
            "k": 3,
            "score_threshold": 0.3
            }
    ) 
    qa = RetrievalQA.from_chain_type(
        llm = llm,
        chain_type = "stuff",
        retriever = retriever,
    )
    response = qa.invoke(question)
    return response
```
### 3.6、整合以上全部内容及爬虫并运行
```python
def combine_all():
    keyword = input("输入搜索词：")
    result = part_get_docs(keyword = keyword)
    print (result)
    response = part_RAG_LLM(question = f"根据你所阅读的文件，分别给各个文件和{keyword}之间的关系排名。你的回答中只需要包含排名。")
    print (response)

combine_all()
```
这样，一个根据关键词自动下载网页并部署知识库的程序就完成了。
最后将返回一个与txt文件相关的排名，由于txt文件指向网页本身，也可用作网页相关度排名。
### 3.7、简要评价
#### 3.7.1、该项目的局限性
该项目需要经历数个复杂的步骤，运行效率低下。且文本嵌入的效果很大程度上会取决于选用的嵌入模型、切分的文本块大小、检索链的参数设置等，未必比直接将文本内容输入语言模型的效果好。
#### 3.7.2、该项目的前景
建立知识库可以很大程度上帮助解决目前语言模型普遍存在的”幻觉“（即语言模型捏造不存在的事情）现象。如果在网页爬取、页面筛选、嵌入模型选用、参数设置等方面继续迭代优化，应该能实现”通过简单输入一个关键词就能建立庞大知识库“的功能。
