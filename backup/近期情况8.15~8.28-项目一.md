# 近期实习情况
近期主要进行两个项目：
1. 通过大模型总结行业信息的程序
2. 文本嵌入后进一步建立本地知识库

由于项目内容较多，本文档仅介绍第一个项目及两个项目的通用内容。
项目二的独有内容参见：[近期情况8.15~8.23:项目2](https://poor-noob.github.io/post/jin-qi-qing-kuang-8.15~8.23--xiang-mu-2.html)

**本项目相关程序：**
[行业信息总结.zip](https://github.com/user-attachments/files/16768157/SearchScrapeSummarizeFiles.zip)
## 1、通用部分
由于两个项目均使用爬虫相关代码，故最先介绍。
### 1.1、引入库，设置全局变量
requests用于向页面发送请求，random和time用于模拟延迟，UserAgent用于伪装请求头，BeautifulSoup用于解析页面内容。
全局变量headers用于之后的爬虫请求头设置。
```python
import requests, random, time
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

headers = {
    "User-Agent": UserAgent().random,
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0"
}
```
### 1.2、根据关键词获取页面链接
应用requests和BeautifulSoup库进行相关处理，通过搜索引擎获取与关键词相关的URL列表。
```python
def get_urls(keyword, max_links):
    search_url = f"https://www.baidu.com/s?wd={keyword}"    
    try:
        response = requests.get(search_url, headers=headers)
        time.sleep(random.random() * 4) 

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            urls = []
            for url in soup.select(".t a"):
                link = url.get("href")
                if link:
                    urls.append(link)
                if len(urls) >= max_links:
                    break
            return urls, ""
        
        else:
            return [], f"获取链接失败：{response.status_code}"
        
    except requests.RequestException as e:
        return [], f"请求出错：{e}"
```
### 1.3、根据获取到的链接，读取页面内容
原理同上，但是需要获取页面内的具体内容，且需要去除广告、图片、视频等无效内容。
```python
def scrape_text(urls):
    web_content = []
    for number, url in enumerate(urls, start = 1):
        try:
            html_content = requests.get(url, headers = headers).text
            time.sleep(random.random() * 4)
            
            rough_text = BeautifulSoup(html_content, "html.parser")
            title = rough_text.title.string
            for tag in rough_text(['script', 'style', 'img', 'video', 'audio']):
                tag.decompose()
            for tag in rough_text.find_all(class_=lambda c: c and ('ad' in c or 'advert' in c)):
                tag.decompose()
            cleaned_text = ' '.join(rough_text.get_text().split())
            web_content.append(cleaned_text)
            print(f"页面{number}的内容已读取。\n")

        except requests.RequestException as e:
            print(f"页面{number}的内容读取失败: {e}\n")

    return web_content
```
这样，一个简单的根据关键词进行页面查找和读取的爬虫就完成了。
## 2、项目一：行业信息总结
由于ScrapeGraphAI的官方文档较不详细，且其自带的爬虫爬取能力并不理想，故选择弃用ScrapeGraphAI。
目前选择使用Langchain+Ollama的组合实现行业信息总结的项目需求。
上文中已有的代码及功能将不再赘述，有区别的地方会特别说明。
### 2.1、引入库，设置全局变量
gradio用于制作交互界面，ChatPromptTemplate用于撰写提示词模板，OllamaLLM用于链接Ollama的语言模型。
四份不同的messages提示词模板分别用于不同功能：行业分析、关键词生成、页面内容总结、相关度排序。
```python
import re
import gradio as gr
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

msg = ""
rank = ""
urls_md = ""
keyword_list = []
model = OllamaLLM(model="qwen:7b")

analyze_messages = [
    ("system", "你是一位非常优秀的分析师。你擅长从八大维度分析一个行业：1、行业定义；2、行业分类；3、行业特征；4、发展历程；5、产品链分析；6、市场规模；7、政策分析；8、竞争格局"),
    ("human", "有一个和{vocation}有关的行业，请你尝试从八大维度分析这个行业。你需要针对每个维度总结一个相关短语，每个短语不超过15个字。"),
    ("ai", "好的，我将会从八大维度对该行业进行分析，并针对每个维度总结一个相关短语：")
]

keyword_messages = [
    ("system", "你是一位非常优秀的分析师。你擅长根据已有的对某个行业的简要分析，给出数个最相关的关键词。"),
    ("human", "有一段关于{vocation}相关行业的简要分析: {analysis}，请你给出任意个最相关的关键词。在你思考关键词的时候请考虑到：该关键词将被输入至搜索引擎内，请确保关键词较容易被搜索到。你的回答只需要包含关键词，关键词请用引号括起来。"),
    ("ai", "好的，我将会根据已有的分析，给出三个最相关的关键词：")
]

summarize_pages_messages = [
    ("system", "你是一位非常优秀的助手。你擅长提取文字中的关键信息，并用易懂的语言进行表达。"),
    ("human", "这个页面有什么与{vocation}相关的内容？请为我用中文简要总结。如果你读取到的页面几乎不包含相关的有效内容，只要回复我“该页面似乎没有相关内容”即可。"),
    ("human", "页面内容：{page_content}"),
    ("ai", "好的，以下是该页面的相关总结：")
]

ranking_pages_messages = [
    ("system", "你是一位非常优秀的分析师。你能够根据网页内容，判断该页面与{vocation}是否有关系。"),
    ("human", "以下是数个页面的页面内容，以列表形式给出：\n{web_content}\n这些页面与{vocation}的相关程度高吗？请给它们的相关程度排名。"),
    ("ai", "好的，以下是这些页面的相关程度排名：")
```
### 2.2、创建问答链
通过相同模板函数+不同提示词创建不同的问答链，用途上文已写。
```python
def thinking_chain(messages):
    prompt = ChatPromptTemplate.from_messages(messages)
    chain = prompt | model
    return chain

analyze_chain = thinking_chain(messages=analyze_messages)
keyword_chain = thinking_chain(messages=keyword_messages)
summarize_chain = thinking_chain(messages=summarize_pages_messages)
ranking_chain = thinking_chain(messages=ranking_pages_messages)
```
### 2.3、激活问答链
通过定义相关的函数应用已有问答链。
获取搜索关键词的函数：
```python
def get_keyword_list(vocation):
    analysis = analyze_chain.invoke({"vocation": vocation})
    print(analysis)

    keyword_string = keyword_chain.invoke({"vocation": vocation, "analysis": analysis})
    print(keyword_string)

    keyword_list = re.findall(r'"(.*?)"', keyword_string)
    keyword_list.append(vocation)
    print(keyword_list)

    return keyword_list
```
获取页面总结和相关度排序的函数：
```python
def chat(vocation, search_engine_url, max_links, keyword_list):
    global msg, rank, urls_md

    if not keyword_list:
        keyword_list.append(vocation)

    for keyword in keyword_list:
        print("当前关键词为：" + keyword + "\n")
        urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)

        for retries in range(3):
            if urls:
                web_content, urls_md = scrape_text(urls)
                print(f"为您找到{len(urls)}个页面！")
                msg += f"关键词：{keyword}\n为您找到{len(urls)}个页面！\n"

                for number, page_content in enumerate(web_content, start=1):
                    response = summarize_chain.invoke({"vocation": vocation, "page_content": page_content})
                    print(f"相关页面{number}的内容：\n" + str("\n".join(web_content)) + "\n\n\n")
                    msg += f"{keyword}相关页面{number}的内容总结：\n\n" + response + "\n\n"
                    print(f"相关页面{number}的内容总结：\n\n" + response + "\n")

                rank = ranking_chain.invoke({"vocation": vocation, "web_content": web_content})
                print(f"相关页面排名：\n{rank}\n")

                break

            elif problem:
                urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)
                msg += problem + "重试中...\n"
                print(problem + "重试中...")

            else:
                urls, problem = get_urls(keyword=keyword, search_engine_url=search_engine_url, max_links=max_links)
                msg += "获取链接失败：未知错误。重试中...\n"
                print("获取链接失败：未知错误。重试中...")

            if retries == 2:
                msg += "错误：当前关键词重试次数过多\n"
                print("错误：当前关键词重试次数过多")
                urls_md += f"[Null](https://127.0.0.1)\n"
                return msg, rank, urls_md
            
    return msg, rank, urls_md
```
### 2.4 交互界面制作
封装上述代码中定义的函数，返回到gradio界面中实现交互。
```python
with gr.Blocks() as gradio_gui:
    def search_engine_url_options():
        return ["baidu", "bing", "google"]
    
    with gr.Column():
        search_engine_url_choices = gr.Dropdown(choices=search_engine_url_options(), label="搜索引擎：", value="baidu")

        search_vocation = gr.Textbox(label="请输入您要搜索的行业：", value="分析师")

        search_max_links = gr.Slider(minimum=1, maximum=10, step=1, label="每个关键词的搜索页面数：")

        start_keyword_generate_button = gr.Button("生成关键词")

        search_keyword = gr.CheckboxGroup(label="相关关键词：", choices=["金融服务业", "数字化进程", "政策利率资本管制", "分析师"])
        
        with gr.Row():
            start_search_button = gr.Button("开始检索")

            clear_button = gr.Button("清空")

        with gr.Row():
            with gr.Column(scale=3):
                search_response = gr.Textbox(label="检索结果：", lines=20, placeholder="这里显示检索结果和总结。", visible=True)

            with gr.Column(scale=1):
                search_rank = gr.Textbox(label="页面度排名：", lines=10, placeholder="这里显示页面度排名。")

        search_urls = gr.Textbox(label="相关链接：", lines=10, placeholder="这里显示相关的页面链接。", visible=True)

        def clear_response():
            return gr.update(value="", visible=False)

        def generate_keyword(search_vocation):
            keyword_list = get_keyword_list(search_vocation)
            return gr.update(choices=keyword_list)
        
        def processing(message, search_engine_url, search_max_links, keyword_list):
            response, rank, urls_md = chat(message, search_engine_url, search_max_links, keyword_list)
            return response, rank, urls_md
        
    start_keyword_generate_button.click(generate_keyword, inputs=search_vocation, outputs=search_keyword)
    start_search_button.click(processing, inputs=[search_vocation, search_engine_url_choices, search_max_links, search_keyword], outputs=[search_response, search_rank, search_urls])
    clear_button.click(clear_response, outputs=[search_response, search_rank, search_urls])

gradio_gui.launch()
```
这样，一个用于行业相关内容检索的小程序就完成了，以下为界面实例：
#### 基础界面：
![image](https://github.com/user-attachments/assets/6db2a95c-c97c-4182-8c72-f2ae574ef9cb)
#### 可自定义搜索引擎：
<img width="487" alt="234f75ad343b4e2bdaf2d6c5487809b" src="https://github.com/user-attachments/assets/237fefea-e348-4422-86bc-9e211994b47a">

#### 返回页面总结、页面排名及超链接：
<img width="1103" alt="5d2700744b7407d3aef92a3283da719" src="https://github.com/user-attachments/assets/08fa9de0-454a-426d-9fa3-d00bfeae7fd0">

## 3、简要评价
该程序目前在稳定运行方面尚有欠缺，爬虫的性能依然存在相当的改进空间。此外，提示词的撰写也并非完美，对页面相关度的排序还有相当的优化空间。

