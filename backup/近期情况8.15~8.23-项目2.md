# 近期实习情况
近期主要进行两个项目：
1. 通过大模型总结行业信息的程序
2. 文本嵌入后进一步建立本地知识库

由于项目内容较多，本文档仅介绍项目二独有内容。
项目一及两个项目的通用内容参见：[近期情况8.15~8.28——项目一](https://poor-noob.github.io/post/jin-qi-qing-kuang-8.15~8.28%E2%80%94%E2%80%94-xiang-mu-1.html)

## 1、项目二：嵌入+本地知识库建立
本项目来源于项目一的最初解决方案。由于运行效率和运行效果都较为一般，现已不作为项目一的解决方案进行开发，而是作为另一方向上的探索，以供未来参考。
### 1.1、引入库，设置全局变量
os用于执行对计算机的相关操作，如创建、写入目录。
DirectoryLoader用于读取文件路径，TextLoader用于阅读文件本身。
RetrievalQA用于创建检索链。
CharacterTextSplitter用于切分文件中的大段文字，便于进一步处理。
HuggingFaceEmbeddings用于使用从HuggingFace上下载的本地嵌入模型进行文本嵌入。
ChatOllama用于链接Ollama的语言模型进行对话。
Chroma用于储存嵌入文本，创建向量库。
path_dict定义文档读取路径和向量库存储路径，embedding_model_dict定义嵌入模型路径。
```python
import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma

llm = ChatOllama(model="qwen:7b")

path_dict = {
    "docs_path": "Path/To/Your/Dict/docs",
    "chroma_path": "Path/To/Your/Dict/VectorStore"
}

embedding_model_dict = {
    "ernie-tiny": "Path/To/Your/Dict/ernie-3.0-nano-zh",
    "ernie-base": "Path/To/Your/Dict/ernie-3.0-base-zh",
    "text2vec": "Path/To/Your/Dict/text2vec-large-chinese",
    "text2vec2": "Path/To/Your/Dict/sbert-base-chinese-nli",
    "text2vec3": "Path/To/Your/Dict/text2vec-base-chinsese"
}
```
### 1.2、根据页面链接获取页面内容并下载为txt文档
```python
def scrape_clean_download(urls, download_path):
    number = 1
    messages = []
    for url in urls:
        try:
            html_content = requests.get(url, headers = headers).text
            time.sleep(random.random()*5)
            rough_text = BeautifulSoup(html_content, "html.parser")
            for tag in rough_text(['script', 'style', 'img', 'video', 'audio']):
                tag.decompose()
            for tag in rough_text.find_all(class_=lambda c: c and ('ad' in c or 'advert' in c)):
                tag.decompose()
            cleaned_text = ' '.join(rough_text.get_text().split())
            path = os.path.join(download_path, f"webpage{number}.txt")
            try:
                with open(path,"x",encoding="utf-8") as file:
                    file.write(cleaned_text)
                messages.append(f"网页{number}的内容已保存到{path}。")
            except FileExistsError:
                messages.append(f"{path}文件已存在，无法保存。")
            finally:
                number += 1
        except:
            pass
    return "\n".join(messages)

def part_get_docs(keyword):
    if get_url(keyword = keyword)[0]:
        return scrape_clean_download(urls = get_url(keyword = keyword)[0], download_path = path_dict["docs_path"])
    else:
        return get_url(keyword = keyword)[1]
```
### 1.3、切分文档
```python
def split_docs():
    docs = DirectoryLoader(path_dict["docs_path"], loader_cls = TextLoader, loader_kwargs = {"autodetect_encoding": True}, use_multithreading=True)
    splited_docs = CharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200).split_documents(docs.load())
    return splited_docs
```
### 1.4、设置嵌入模型，建立向量库
```python
def set_embedding(model_name="text2vec"):
    try:
        embedding = HuggingFaceEmbeddings(
                model_name=embedding_model_dict[model_name],
                model_kwargs={"device": "cuda:0"},
                encode_kwargs={'normalize_embeddings': False}
            )
    except AssertionError as e:
        embedding = HuggingFaceEmbeddings(
                model_name=embedding_model_dict[model_name],
                model_kwargs={"device": "cpu"},
                encode_kwargs={'normalize_embeddings': False}
            )
    return embedding

def store_chroma(docs):
    db = Chroma.from_documents(documents = docs, embedding = set_embedding(), persist_directory = path_dict["chroma_path"])
    db.persist()
    return db
```
### 1.5、整合前两步内容并创建检索链
```python
def part_RAG_LLM(question):
    if not os.path.exists(path_dict["chroma_path"]):
        os.mkdir(path_dict["chroma_path"])
        db = store_chroma(docs = split_docs())
    else:
        db = Chroma(embedding_function = set_embedding(), persist_directory = path_dict["chroma_path"])

    retriever = db.as_retriever(
        search_type = "similarity_score_threshold",
        search_kwargs = {
            "k": 3,
            "score_threshold": 0.3
            }
    ) 
    qa = RetrievalQA.from_chain_type(
        llm = llm,
        chain_type = "stuff",
        retriever = retriever,
    )
    response = qa.invoke(question)
    return response
```
### 1.6、整合以上全部内容及爬虫并运行
```python
def combine_all():
    keyword = input("输入搜索词：")
    result = part_get_docs(keyword = keyword)
    print (result)
    response = part_RAG_LLM(question = f"根据你所阅读的文件，分别给各个文件和{keyword}之间的关系排名。你的回答中只需要包含排名。")
    print (response)

combine_all()
```
这样，一个根据关键词自动下载网页并部署知识库的程序就完成了。
最后将返回一个与txt文件相关的排名，由于txt文件指向网页本身，也可用作网页相关度排名。
## 2、简要评价
### 2.1、该项目的局限性
该项目需要经历数个复杂的步骤，运行效率低下。且文本嵌入的效果很大程度上会取决于选用的嵌入模型、切分的文本块大小、检索链的参数设置等，未必比直接将文本内容输入语言模型的效果好。
### 2.2、该项目的前景
建立知识库可以很大程度上帮助解决目前语言模型普遍存在的”幻觉“（即语言模型捏造不存在的事情）现象。如果在网页爬取、页面筛选、嵌入模型选用、参数设置等方面继续迭代优化，应该能实现”通过简单输入一个关键词就能建立庞大知识库“的功能。
## 本页面相关程序：
[爬虫+文本嵌入+本地知识库程序](https://github.com/user-attachments/files/16766494/simple_rag.zip)
[更稳定的本地知识库建立，但不包含爬虫](https://github.com/user-attachments/files/16768116/doc_RAG.zip)

[